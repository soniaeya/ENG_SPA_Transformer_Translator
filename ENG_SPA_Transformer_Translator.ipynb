{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/soniaeya/ENG_SPA_Transformer_Translator/blob/main/ENG_SPA_Transformer_Translator.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q1ojcRojUI2u"
      },
      "source": [
        "## Assigment 2: Transformers for Translation üôä\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FbsZ8_HXD1bV"
      },
      "source": [
        "Have you ever wondered how applications like Google Translate or language translation features in social media platforms work? Behind these impressive technologies are sophisticated machine learning models that can understand and translate text between different languages. One of the most powerful and groundbreaking models used for this purpose is the Transformer model.\n",
        "\n",
        "In this assignment, you will step into the shoes of an AI researcher and engineer to create your own Transformer model for translating text from English to Spanish. This journey will not only enhance your understanding of machine learning and deep learning but also give you hands-on experience with state-of-the-art techniques in natural language processing."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DkOvjxXZEmRn"
      },
      "source": [
        "Let's start by downloading important libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "SWDs3o4jvnzn",
        "outputId": "244fad2e-d680-4be5-ce40-55f098126e19"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.12/dist-packages (4.0.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from datasets) (3.20.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from datasets) (2.0.2)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.12/dist-packages (from datasets) (2.32.4)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.12/dist-packages (from datasets) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets) (3.6.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2025.3.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.36.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from datasets) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from datasets) (6.0.3)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.13.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.24.0->datasets) (1.2.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (2025.11.12)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.22.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Collecting evaluate\n",
            "  Downloading evaluate-0.4.6-py3-none-any.whl.metadata (9.5 kB)\n",
            "Requirement already satisfied: datasets>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from evaluate) (4.0.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from evaluate) (2.0.2)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.12/dist-packages (from evaluate) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from evaluate) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.12/dist-packages (from evaluate) (2.32.4)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.12/dist-packages (from evaluate) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from evaluate) (3.6.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.12/dist-packages (from evaluate) (0.70.16)\n",
            "Requirement already satisfied: fsspec>=2021.05.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (2025.3.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from evaluate) (0.36.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from evaluate) (25.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from datasets>=2.0.0->evaluate) (3.20.0)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets>=2.0.0->evaluate) (18.1.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from datasets>=2.0.0->evaluate) (6.0.3)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (3.13.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.7.0->evaluate) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.7.0->evaluate) (1.2.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->evaluate) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->evaluate) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->evaluate) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->evaluate) (2025.11.12)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->evaluate) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->evaluate) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->evaluate) (2025.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.22.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.17.0)\n",
            "Downloading evaluate-0.4.6-py3-none-any.whl (84 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: evaluate\n",
            "Successfully installed evaluate-0.4.6\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.57.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers) (3.20.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.36.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2025.11.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers) (2.32.4)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.7.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.2.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2025.11.12)\n"
          ]
        }
      ],
      "source": [
        "!pip install datasets\n",
        "!pip install evaluate\n",
        "!pip install transformers\n",
        "!pip install bert_score\n",
        "!pip install rouge_score"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t4o0FXVqEsMb"
      },
      "source": [
        "For this assignment we are using the Opus Book dataset (read more about it [here](https://huggingface.co/datasets/Helsinki-NLP/opus_books) ). This dataset easily found in huggingface fits perfectly for our machine translation task."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DnyUFsrLuKen"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "books = load_dataset(\"opus_books\", \"en-es\")\n",
        "books = books[\"train\"].train_test_split(test_size=0.2)\n",
        "train_dataset = books[\"train\"]['translation']\n",
        "test_size = int(len(books[\"test\"])/2)\n",
        "val_dataset = books[\"test\"][:test_size]['translation']\n",
        "test_dataset = books[\"test\"][test_size:]['translation']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6e9gOL_wFNUV"
      },
      "source": [
        "Just to have an idea let's have a quick peak on how our dataset looks like"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset[0]"
      ],
      "metadata": {
        "id": "L3Vbf3bcydZc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oL3XE7WHMWiw"
      },
      "source": [
        "Since we don't want to take 8 hours training let's trim our dataset a bit (although this might lead to underperformance, feel free to use the complete dataset if you have the computing power)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dJE2Rf46N9s7"
      },
      "source": [
        "SUGESTION: start with a small dataset to debug your code and increase it gradually (same applies with number of epochs, batch size, test set size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6zbZtQGnOS42"
      },
      "outputs": [],
      "source": [
        "train_dataset = train_dataset[:30000]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JxGkuEIgsKiU"
      },
      "source": [
        "### Preprocessing\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NpG6_OkVFW6L"
      },
      "source": [
        "Preprocessing is an important part of NLP. This allows us to clean and standarize our data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F-27GvDhsMR6"
      },
      "outputs": [],
      "source": [
        "import string\n",
        "import re\n",
        "def preprocess_data(text):\n",
        "  \"\"\" Method to clean text from noise and standarize text.\n",
        "  Arguments\n",
        "  ---------\n",
        "  text : List of String\n",
        "     Text to clean\n",
        "  Returns\n",
        "  -------\n",
        "  text : String\n",
        "      Cleaned and joined text\n",
        "  \"\"\"\n",
        "\n",
        "  text = text.lower()\n",
        "\n",
        "  text= re.sub(r'[^\\w\\s√Å√â√ç√ì√ö√°√©√≠√≥√∫√º√ú√±√ë]', '', text) #remove any punctuation or special characters without taking off accent letters like √≥,√≠,etc..\n",
        "  text = re.sub(r\"\\s+\", \" \", text).strip() #to remove extra space (I THINK CAN REMOVE)\n",
        "\n",
        "  return text\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "assert \"para ti es una cuesti√≥n de tozudez dijo ana de repente al encontrar una palabra que definiera justamente los pensamientos y el sentir de vronsky un calificativo para aquella expresi√≥n de su rostro que tanto la irritaba\"==preprocess_data(\"‚ÄìPara ti es una cuesti√≥n de tozudez ‚Äìdijo Ana de repente, al encontrar una palabra que definiera justamente los pensamientos y el sentir de Vronsky, un calificativo para aquella expresi√≥n de su rostro que tanto la irritaba‚Äì.\"), \"Check errors in preprocessing\"\n",
        "print(\"Good Job!\")"
      ],
      "metadata": {
        "id": "FY7n-3zM4BFM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mnLE32PKFoWk"
      },
      "source": [
        "For an easier training structure, it is useful to format our training and validation sets. The following function should help with this."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TbUiwe6JvZ_c"
      },
      "outputs": [],
      "source": [
        "def create_dataset(dataset,source_lang,target_lang):\n",
        "  \"\"\" Method to create a dataset from a list of text.\n",
        "  Arguments\n",
        "  ---------\n",
        "  text : List of String\n",
        "     Text from dataset\n",
        "  source_lang : String\n",
        "     Source language\n",
        "  target_lang : String\n",
        "     Target language\n",
        "  Returns\n",
        "  -------\n",
        "  new_dataset : Tuple of String\n",
        "      Source and target text in format (source, target)\n",
        "  \"\"\"\n",
        "  new_dataset=[]\n",
        "  #TODO: iterate through dataset extract source and target dataset and preprocess them creating a new clean dataset with the correct format\n",
        "\n",
        "  return new_dataset\n",
        "\n",
        "training_set=create_dataset(train_dataset,'en','es')\n",
        "validation_set=create_dataset(val_dataset,'en','es')\n",
        "test_set=create_dataset(test_dataset,'en','es')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6AN8JIQ8Y71o"
      },
      "source": [
        "### Model Creation\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b-IJWkSbzZeT"
      },
      "source": [
        "Now that our data is ready, we can get started. Let's start by creating our Sequence to Sequence Transformer model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MpTlmB1t3cii"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "class TransformerModel(nn.Module):\n",
        "    def __init__(self, src_vocab_size, tgt_vocab_size, d_model, nhead, num_encoder_layers, num_decoder_layers, dim_feedforward,dropout):\n",
        "        super(TransformerModel, self).__init__()\n",
        "        self.src_embedding = # Embedding layer for source language\n",
        "        self.tgt_embedding = # Embedding layer for target language\n",
        "        self.transformer = # Transformer model with it's attributes (see pytorch documentation)\n",
        "        self.fc = # Last linear layer\n",
        "\n",
        "\n",
        "\n",
        "    def positional_encoding(self, d_model, maxlen = 5000):\n",
        "        \"\"\"Method to create a positional encoding buffer.\n",
        "        Arguments\n",
        "        ---------\n",
        "        d_model: int\n",
        "            Embedding size\n",
        "        maxlen: int\n",
        "            Maximum sequence length\n",
        "        Returns\n",
        "        -------\n",
        "        PE: Tensor\n",
        "            Positional encoding buffer\n",
        "        \"\"\"\n",
        "        pos = torch.arange(0, maxlen).unsqueeze(1)\n",
        "        denominator = 10000 ** (torch.arange(0, d_model, 2) / d_model)\n",
        "\n",
        "        #TODO\n",
        "        PE = torch.zeros((maxlen, d_model))\n",
        "        PE[:, 0::2] = # Calculate sin for even positions\n",
        "        PE[:, 1::2] = # Calculate cosine odd positions\n",
        "\n",
        "        PE = PE.unsqueeze(0)  # Add batch dimension\n",
        "\n",
        "        return PE\n",
        "\n",
        "\n",
        "    def forward(self, src, tgt, src_mask=None, tgt_mask=None, src_key_padding_mask=None, tgt_key_padding_mask=None):\n",
        "        \"\"\"Method to forward a batch of data through the model.\"\"\"\n",
        "        #TODO\n",
        "        #pass source and target throught embedding layer\n",
        "        #get src_emb and tgt_emb by adding positional encoder\n",
        "        #add positional encoding to src and tgt\n",
        "        #pass src, tgt and all masks throught transformer\n",
        "        #pass output throught linear layer\n",
        "        return output\n",
        "\n",
        "    def encode(self, src, src_mask):\n",
        "        \"\"\"Method to encode a batch of data through the transformer model.\"\"\"\n",
        "        #TODO\n",
        "        #pass src throught embedding layer\n",
        "        #create positional encoding\n",
        "        #add src_emb and src_pe\n",
        "        #pass src_emb and src_mask throught transformer encoder using self.transformer.encoder\n",
        "\n",
        "        return self.transformer.encoder(src_emb, src_mask)\n",
        "\n",
        "\n",
        "    def decode(self, tgt, memory,tgt_mask):\n",
        "        \"\"\"Method to decode a batch of data through the transformer model.\"\"\"\n",
        "        #TODO\n",
        "        #pass tgt throught embedding layer\n",
        "        #create positional encoding\n",
        "        #add tgt_emb and tgt_pe\n",
        "        #pass tgt_emb and tgt_mask throught transformer encoder using self.transformer.decoder\n",
        "\n",
        "        return self.transformer.decoder(tgt_emb, memory,tgt_mask)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5FodHRx7pRD6"
      },
      "source": [
        "Now that our model is ready, we still need some methods that will come in handy during training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4S8Ys4-OJ4Mj"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "def create_padding_mask(seq):\n",
        "  \"\"\" Method to create a padding mask based on given sequence.\n",
        "  Arguments\n",
        "  ---------\n",
        "  seq : Tensor\n",
        "     Sequence to create padding mask for\n",
        "  Returns\n",
        "  -------\n",
        "  mask : Tensor\n",
        "      Padding mask\n",
        "  \"\"\"\n",
        "  return #boolean matrix that is True when datapoint is equal to 0\n",
        "\n",
        "def create_triu_mask(sz):\n",
        "  \"\"\" Method to create a triangular mask based on given sequence. This is used for the tgt mask in the Transformer model to avoid looking ahead.\n",
        "  Arguments\n",
        "  ---------\n",
        "  seq : Tensor\n",
        "     Sequence to create triangular mask for\n",
        "  Returns\n",
        "  -------\n",
        "  mask : Tensor\n",
        "      Triangular mask\n",
        "  \"\"\"\n",
        "  # TODO\n",
        "  #create triangular mask of size sz x sz\n",
        "  #tranpose mask and cast to float type\n",
        "  #in pytorch the masked objects expect -inf instead of zero. Replace all 0 for -inf and all 1's for 0's\n",
        "  return mask\n",
        "\n",
        "def tokenize_batch(source, targets,tokenizer):\n",
        "  \"\"\" Method to tokenize a batch of data given a tokenizer.\n",
        "  Arguments\n",
        "  ---------\n",
        "  source : List of String\n",
        "     Source text\n",
        "  targets : List of String\n",
        "     Target text\n",
        "  tokenizer : Tokenizer\n",
        "     Tokenizer to use for tokenization\n",
        "  Returns\n",
        "  -------\n",
        "  tokenized_source : Tensor\n",
        "      Tokenized source text\n",
        "  \"\"\"\n",
        "\n",
        "  tokenized_source = tokenizer(source, padding='max_length', max_length=128, return_tensors='pt', truncation=True)\n",
        "\n",
        "  tokenized_targets = tokenizer(targets,  padding='max_length', max_length=128, return_tensors='pt',truncation=True)\n",
        "\n",
        "  return tokenized_source['input_ids'], tokenized_targets['input_ids']\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "a=create_triu_mask(5)\n",
        "b= torch.tensor([[0., float('-inf'), float('-inf'), float('-inf'), float('-inf')],\n",
        "        [0., 0., float('-inf'), float('-inf'), float('-inf')],\n",
        "        [0., 0., 0., float('-inf'), float('-inf')],\n",
        "        [0., 0., 0., 0., float('-inf')],\n",
        "        [0., 0., 0., 0., 0.]])\n",
        "assert torch.equal(a,b), \"Issues with create_triu_mask\"\n",
        "print(\"Good Job!\")"
      ],
      "metadata": {
        "id": "WN3GSm1t9JIV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f7qBZbprY_a0"
      },
      "source": [
        "### Training\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2pKk5iK1jJAB"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "tokenizer=AutoTokenizer.from_pretrained('FacebookAI/xlm-roberta-base')\n",
        "PAD_IDX = tokenizer.pad_token_id #for padding\n",
        "BOS_IDX = tokenizer.bos_token_id #for beggining of sentence\n",
        "EOS_IDX = tokenizer.eos_token_id #for end of sentence\n",
        "\n",
        "model = TransformerModel(tokenizer.vocab_size, tokenizer.vocab_size,512, 8, 3, 3, 256,0.1).to(device)\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-9)\n",
        "loss_function = torch.nn.CrossEntropyLoss(ignore_index=PAD_IDX)\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(training_set, batch_size=8, shuffle=True) #change batch size based on your reasources\n",
        "validation_loader = torch.utils.data.DataLoader(validation_set, batch_size=8, shuffle=False) #change batch size based on your reasources"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "tokenizer=AutoTokenizer.from_pretrained('FacebookAI/xlm-roberta-base')"
      ],
      "metadata": {
        "id": "EkYklUEI5AdT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tF7sRoh7_VBe"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "from tqdm import tqdm\n",
        "\n",
        "def train_epoch(model,train_loader,tokenizer):\n",
        "    model.train()\n",
        "    losses = 0\n",
        "\n",
        "    for src, tgt in tqdm(train_loader):\n",
        "        src, tgt = tokenize_batch(src, tgt, tokenizer)\n",
        "        src = src.to(device)\n",
        "        tgt = tgt.to(device)\n",
        "\n",
        "        tgt_input = tgt[:,:-1]\n",
        "\n",
        "        #TODO\n",
        "        src_mask = #creat src_mask this is basically a matrix of 0s of shape Sequence x Sequence (see https://pytorch.org/docs/stable/generated/torch.nn.Transformer.html)\n",
        "        tgt_mask = #create triangular mask for target\n",
        "\n",
        "        src_padding_mask = #create padding mask for src\n",
        "        tgt_padding_mask = #create padding mask for tgt\n",
        "\n",
        "        logits = #pass it through model\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        tgt_out = tgt[:,1:]\n",
        "        loss = loss_function(logits.reshape(-1, logits.shape[-1]), tgt_out.reshape(-1))\n",
        "        loss.backward()\n",
        "\n",
        "        optimizer.step()\n",
        "        losses += loss.item()\n",
        "\n",
        "    return losses / len(list(train_loader))\n",
        "\n",
        "\n",
        "def evaluate(model,val_dataloader ):\n",
        "    model.eval()\n",
        "    losses = 0\n",
        "    with torch.no_grad():\n",
        "      for src, tgt in tqdm(val_dataloader):\n",
        "          src, tgt = tokenize_batch(src, tgt, tokenizer)\n",
        "          src = src.to(device)\n",
        "          tgt = tgt.to(device)\n",
        "\n",
        "          tgt_input = tgt[:,:-1]\n",
        "\n",
        "          #do the same as in Train\n",
        "\n",
        "          tgt_out = tgt[:,1:]\n",
        "          loss = loss_function(logits.reshape(-1, logits.shape[-1]), tgt_out.reshape(-1))\n",
        "          losses += loss.item()\n",
        "\n",
        "    return losses / len(list(val_dataloader))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YOXBEOakYYMy"
      },
      "source": [
        "Now we can start training! Keep in mind this code is very demanding computationally, it has been set to 10 epochs (which can take up to 4 hours) but feel free to change this value depending on your resources, in this case the more epochs you can execute the better üòÄ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "55HSxWxs_VLV"
      },
      "outputs": [],
      "source": [
        "def train(model, epochs, train_loader,validation_loader ):\n",
        "  for epoch in range(1, epochs+1):\n",
        "        train_loss = train_epoch(model,train_loader, tokenizer)\n",
        "        val_loss = evaluate(model,validation_loader)\n",
        "        print((f\"Epoch: {epoch}, Train loss: {train_loss:.3f}, Val loss: {val_loss:.3f}\"))\n",
        "\n",
        "train(model, 10, train_loader,validation_loader)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d-fyWwVLZCOl"
      },
      "source": [
        "### Testing\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5SFHr7KgZEXg"
      },
      "source": [
        "We will use three different evaluation metrics to see our model's test performance: [Bert Score](https://huggingface.co/spaces/evaluate-metric/bertscore), [Meteor](https://huggingface.co/spaces/evaluate-metric/meteor) and [Rouge](https://huggingface.co/spaces/evaluate-metric/rouge). Please access their hugging face documentation to know how to implement them."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lAwdvSLIIJ2K"
      },
      "outputs": [],
      "source": [
        "from evaluate import load\n",
        "bertscore = load(\"bertscore\")\n",
        "rouge = load('rouge')\n",
        "meteor = load('meteor')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a4VJA2vfZx4o"
      },
      "source": [
        "Implement greedy decode seen in class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M854C4naGHW6"
      },
      "outputs": [],
      "source": [
        "# function to generate output sequence using greedy algorithm\n",
        "def greedy_decode(model, src, src_mask, max_len, start_symbol):\n",
        "    src = src.to(device)\n",
        "    src_mask = src_mask.to(device)\n",
        "    memory = #pass src through encoder\n",
        "    ys = torch.ones(1, 1).fill_(start_symbol).type(torch.long).to(device)\n",
        "    for i in range(max_len-1):\n",
        "        memory = memory.to(device)\n",
        "        tgt_mask = #create triangular mask\n",
        "        out = #pass through decoder\n",
        "\n",
        "        prob = model.fc(out[:, -1])\n",
        "\n",
        "        _, next_word = #get next word based on probabilities (remember to use .item())\n",
        "\n",
        "        ys = torch.cat([ys,\n",
        "                        torch.ones(1, 1).type_as(src.data).fill_(next_word)], dim=1)\n",
        "        if next_word == EOS_IDX:\n",
        "            break\n",
        "    return ys\n",
        "\n",
        "def translate(model: torch.nn.Module, src_sentence: str, tokenizer):\n",
        "    model.eval()\n",
        "    src, _ = tokenize_batch(src_sentence, \"\", tokenizer)\n",
        "    src = src.to(device)\n",
        "    num_tokens = src.shape[1]\n",
        "    src_mask = (torch.zeros(num_tokens, num_tokens)).type(torch.float).to(device)\n",
        "    tgt_tokens = greedy_decode(\n",
        "        model,  src, src_mask, max_len= int(num_tokens * 1.2 ), start_symbol=BOS_IDX).flatten()\n",
        "    return tokenizer.decode(tgt_tokens, skip_special_tokens=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NXPxCVAuF77g"
      },
      "outputs": [],
      "source": [
        "print(translate(model, \"hello how are you today\",tokenizer))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_set = test_set[:1000]"
      ],
      "metadata": {
        "id": "jJBM-SDJ2Uox"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3Kqprzu9z5RH"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "# you can also trim test_loader\n",
        "def test(test_loader, model, tokenizer, device, max_length=200):\n",
        "  \"\"\"Method to test our model using best score and meteor metric.\n",
        "  Arguments\n",
        "  ---------\n",
        "  test_loader: Dataloader\n",
        "    Dataloader that holds test set\n",
        "  model: nn.Module\n",
        "    trained Machine Translation model\n",
        "  tokenizer:\n",
        "  \"\"\"\n",
        "  precision = 0\n",
        "  recall = 0\n",
        "  f1 = 0\n",
        "  meteor_metric = 0\n",
        "  for src, target in test_loader:\n",
        "    #Use translade method to evaluate our model\n",
        "    results_bert = #get results bert\n",
        "    results_meteor = #get results meteo\n",
        "    precision += #get precision of results_bert\n",
        "    recall += #get recall of results_bert\n",
        "    f1 += #get f1 of results_bert\n",
        "    meteor_metric+= #get meteor metric of results_meteor\n",
        "  return precision / len(test_loader), recall / len(test_loader), f1 / len(test_loader), meteor_metric / len(test_loader)\n",
        "\n",
        "test(test_set, model, tokenizer, device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Np_Eg1L-sgK5"
      },
      "source": [
        "## Let's experiment!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0u54CCcr74NK"
      },
      "source": [
        "Pick 2 experiments out these 3:\n",
        "1. Play with a hyperparameter of your choice to measure its effect on the translation.\n",
        "\n",
        "2. Train an inverse translator from spanish to english and compare the performance.\n",
        "\n",
        "3. Compare the results of your model with the performance of using the T5 pretrained model. This [tutorial](https://huggingface.co/docs/transformers/en/tasks/translation) on using T5 for machine translation might come in handy.\n",
        "\n",
        "\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}